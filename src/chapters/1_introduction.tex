Text Summarization is the process of shortening a larger text without
losing any key information.
But manually summarizing longer texts is really time-consuming
and counter-productive.
So developing an Automatic Text Summarization (ATS) system
that would process larger documents and summarize them
automatically~\cite{Widyassari-2022-rev-ats-tech-met} is really necessary.
This is a major area of natural language processing (NLP) research
which is progressing very rapidly.
This task has become more and more important in the current digital age,
where the amount of textual data grows exponentially in many
fields~\cite{2015-Forbes-80-created-last-2-years},
such as news, legal documents, health reports, research papers,
and social media content.
ATS techniques allow users to quickly get the essential information
without needing to read through large amounts of
text~\cite{wafaa-2021-summary-comprehensive-review}.
ATS is used in many fields, from automatic
news summarization, content filtering, and recommendation
systems to assisting legal professionals in going through
long documents and researchers in reviewing academic papers.
It can also play a critical role in personal assistants and chatbots,
providing condensed information to users quickly
and efficiently~\cite{tas-2017-rev-text-sum-2}.\\

There are two main types of automatic text summarization: extractive
and abstractive~\cite{tas-2017-rev-text-sum-2}.
Extractive summarization, which is the focus of this paper, works by
selecting the best sentences or phrases directly
from the source document, maintaining the original wording and
sentence structure~\cite{moratanch-2017-extractive-review}.
In contrast, abstractive summarization involves generating new
sentences to capture the meaning of the text, Similar to human made
summarization~\cite{Moratanch-2016-abstractive-rev}.
For text summarization, extractive methods are widely used because of its
simplicity and effectiveness, especially for languages with
limited NLP resources~\cite{gupta-2010-extractive-rev}.\\

There are a lot of different ways to achieve extractive summarization.
A commonly used method for extractive text summarization
is graph-based summarization~\cite{wafaa-2021-summary-comprehensive-review}.
This method represents the sentences of a document as
nodes of a graph, and the edges between them are weighted by the
similarity between the sentences~\cite{wafaa-2021-summary-comprehensive-review}.
Popular algorithms like LexRank~\cite{Erkan-lexRank-2004}
and TextRank~\cite{mihalcea-2004-textrank} build graphs based
on cosine similarity between sentences and
apply ranking algorithms such as PageRank~\cite{page-PageRank-1999} in
case of LexRank~\cite{Erkan-lexRank-2004} or Random
Walk in case of TextRank~\cite{mihalcea-2004-textrank} to
determine which sentences are the most important.
These sentences are then selected to make the summary.
Graph-based methods offer a robust way to capture
sentence importance and relationship, ensuring that the
extracted summary covers the key information while
minimizing redundancy~\cite{wafaa-2021-summary-comprehensive-review}.\\

Clustering-based approaches are a subset of graph-based approach
to extractive text summarization.
Here, sentences are grouped into clusters based on their
semantic similarity, and one representative sentence from each cluster is chosen
to form the summary~\cite{Mohan-2022-topic-modeling-rev-clustering}.
Clustering reduces redundancy by ensuring that similar
sentences are grouped together and that only the most
representative sentence is selected.
This method is effective in documents that cover multiple
topics or subtopics, as it allows the summary to touch on
each area without being repetitive.\\

For Bengali, a low-resource language, early attempts at text
summarization relied on traditional methods like TF-IDF
(Term Frequency-Inverse Document Frequency)
scoring~\cite{Akter-2017-tfidf-3,das-2022-tfidf,
    sarkar-2012-tfidf,sarkar-2012-tfidf-2}.
These approaches, while simple, faced challenges in
capturing the true meaning of sentences, as they treated words as
isolated terms~\cite{tas-2017-rev-text-sum-2}.
Graph-based methods introduced improvements by
incorporating sentence similarity, but they were still limited by the
quality of the embeddings used for the Bengali language.
With the advent of word embedding models like
FastText~\cite{grave-etal-2018-fasttext}, which supports over 157
languages, including Bengali, it became possible to
represent words in a Vector Space Model, thus enabling
more accurate sentence similarity calculations.
However, existing models that use word embeddings,
such as~\citeauthor{roychowdhury-etal-2022-spectral-base}â€™s
\cite{roychowdhury-etal-2022-spectral-base} Sentence
Average Similarity-based Spectral Clustering (SASbSC) method,
encountered issues when averaging word vectors to represent sentence meaning.
This method failed in most cases because words in a sentence
are often complementary rather than being similar, leading
to inaccurate sentence representations when averaging their vectors.
As a result, important word-to-word relationships between
sentences were lost, reducing the effectiveness of the method.\\

In this paper, we propose a new approach to address these challenges.
Our method improves upon previous
attempts~\cite{roychowdhury-etal-2022-spectral-base}
by focusing on the individual similarity between words
in sentences rather than averaging word vectors.
Here, to get the similarity between two sentences,
we took the gaussian similarity for each word of a sentence and
the word that is the Most Similar Word from the other sentence.
This method captures the similarity of meaning between
two sentences more accurately.
By applying Gaussian similarity to the Most Similar
Word Distance ($D_{msw}$) values, we build an affinity matrix that
better reflects sentence closeness which can be proved
by the effectiveness of the model (Table~\ref{tab:result_comparison-1}).
We then apply spectral clustering on this matrix to
group similar sentences together and use TF-IDF to select the
most representative sentences from each cluster.
This approach reduces redundancy and improves the
quality of the summary by selecting sentences that are not only
relevant but also diverse.
This method works really well for Bengali on four
diverse datasets (Figure~\ref{fig:radarchart}).
It consistently outperforms other graph-based methods
like BenSumm~\cite{chowdhury-etal-2021-tfidf-clustering},
SASbSC~\cite{roychowdhury-etal-2022-spectral-base},
LexRank~\cite{Erkan-lexRank-2004}.
It also performs similarly well in other low resource languages such as
Hindi, Marathi and Turkish (Table~\ref{tab:other_language}).
These are the only other low resource languages
where we found reliable evaluation datasets and tested our model on them.
The search process was not exhaustive due to our language barrier.
The whole process of summarization is shown in the Process Flow Diagram (Figure~\ref{fig:process-flow-diagram})\\

\begin{figure}
    \centering
    \input{figs/0101_process_flow_diagram}
    \caption{Process Flow Diagram}
    \label{fig:process-flow-diagram}
\end{figure}

The main contributions of this paper are:
(I) Creating a new way to calculate similarity
between two sentences.
(II) Contributes a novel methodology for extractive
text summarization for the Bengali language;
By improving sentence similarity calculations and
enhancing clustering techniques.
(III)  It offers a generalizable solution for creating less
redundant and information rich summaries across languages.
(V) It provides a publicly available high quality dataset
of 500 human generated summaries.\\

The rest of the paper is organized as follows:
The Literature review and Methodology are described in
section 2 and 3 respectively.
The section 4 illustrates the findings of this work.
The section 5 discusses the findings of the paper in more depth,
and section 6 concludes the paper.