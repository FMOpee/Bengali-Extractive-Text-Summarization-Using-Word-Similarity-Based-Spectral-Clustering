The results presented in the previous sections highlight the effectiveness of the proposed WSbSC model for extractive text summarization in Bengali, as well as its adaptability to other low-resource languages. This section delves into an analysis of the comparative results, the strengths and limitations of the proposed method, and potential areas for further research.\\

As evidenced by the results shown in Table~\ref{tab:result_comparison-1} and Figure~\ref{fig:radarchart}, the WSbSC model consistently outperforms the baseline models, namely BenSumm~\cite{das-2022-tfidf}, LexRank~\cite{Erkan-lexRank-2004}, and SASbSC~\cite{roychowdhury-etal-2022-spectral-base}, across multiple datasets. This performance improvement is largely for the novel approach of calculating sentence similarity. Taking the geometric means of individual word similarities overcomes the problems of averaging vector method. The Gaussian similarity-based approach used in WSbSC provides a more novel and precise method for capturing the semantic relationships between sentences.\\
%The superior performance of WSbSC is especially noticeable in terms of ROUGE scores,
%where it consistently achieves higher F-1 scores across all datasets (Table~\ref{tab:result_comparison-1}).
%This suggests that the proposed method generates summaries that are more faithful to the
%human written reference summaries.
%Additionally, the clustering approach makes sure that sentences with similar information
%are grouped together, allowing the model to pick the most representative sentence from each group.
%This reduces redundancy and increases topic coverage, key components of a good summary.\\

Our proposed strategy is more suited for the sentence similarity calculation than other strategies that can compare two sets of vectors such as Earth Movers Distance (EMD)~\cite{Rubner-19998-emd}, Hausdorff Distance~\cite{hausdorff-1914-hausdorff-distance}, Procrustes Analysis~\cite{Gower-1975-procrustes-distance}. EMD~\cite{Rubner-19998-emd} tries to find the lowest amount of ``work'' needed to transform one set into the other one. It considers adding a new point to a set, removing a point from a set, scaling the whole set, moving a point, rotating the set etc.\ as ``work''. This is very computationally expensive as hundreds of separate possibilities have to be checked for each point in each set. And it also focuses on scaling and rotating, which are not relevant in word vector space. Another method, Hausdorff distance~\cite{hausdorff-1914-hausdorff-distance} takes the worst case scenario and calculates the farthest distance between two points in the two set. It is easily influenced by outliers. It was avoided because words tend to spread out over the whole word space and this would suffer from the same problem as the averaging method. Procrustes Analysis~\cite{Gower-1975-procrustes-distance} tries to find the lowest amount of misalignment after scaling and rotating the two sets. Both of these processes are irrelevant in the context of word vector.\\

On the other hand, the proposed method focuses on Local Point Correspondence between two sets which is more important for words. The Gaussian similarity function captures the proximity of points smoothly, providing continuous feedback on how similar two points are in a normalized way. It is also robust against small outliers because of the use of a soft similarity measure (Gaussian) and geometric mean which helps smooth over small differences in word similarities.\\

One of the key strengths of this proposed method is the reduction of redundancy, which is a common issue in extractive summarization methods. By grouping sentences with similar meanings and selecting a representative sentence from each group, the model ensures that the summary covers a broad range of topics without repeating itself. The use of Spectral clustering is well-suited for the clustering task too because it does not assume a specific cluster shape and can infer the number of clusters using the Eigen gap method. Our proposed model also has an improved sentence similarity calculation technique. Using the geometric mean of individual word similarities offers a more precise measure of how closely two sentences are related. This is a marked improvement over traditional methods that rely on word averaging, which often dilute the semantic meaning of a sentence. Another key strength is that it is found to be scalable across languages. By requiring only a language-specific tokenizer, stop-word list, and word embedding dataset, WSbSC can be easily adapted to other languages, as demonstrated in the experiments with Hindi, Marathi, and Turkish datasets (Table~\ref{tab:other_language}). This makes the model highly versatile and valuable for extractive summarization in low-resource languages.\\

Despite its advantages, the WSbSC model does face some challenges. The model heavily relies on pre-trained word embeddings, which may not always capture the full details of certain domains or newly coined terms. The FastText~\cite{grave-etal-2018-fasttext} dataset used here is heavily reliant on wikipedia for training. Which could introduce some small unforeseen biases. In cases where the word embeddings do not fully have some word of a given document, the model’s performance could degrade as it leaves those words out. The model also does not take into account the order in which words appear in a sentence or when the form special noun or verb groups. So it can be a little naive in some highly specialized fields.\\

The WSbSC model has demonstrated its ability to perform well in low-resource languages such as Hindi, Marathi, and Turkish. Despite differences in language structure, the model’s core methodology remained effective, yielding results that were consistent with the Bengali dataset evaluations. This underscores the potential of WSbSC as a generalizable approach for extractive summarization across different languages, if appropriate pre-processing tools and word embedding datasets are available.\\

The proposed Word Similarity-based Spectral Clustering model represents a significant advancement in Bengali extractive text summarization. Its ability to accurately capture sentence similarity, reduce redundancy, and generalize across languages makes it a valuable tool for summarizing text in low-resource languages. While there are still challenges to be addressed, the results of this study demonstrate the robustness and adaptability of the WSbSC model, offering a promising direction for future research in multilingual extractive summarization.
