The summarization process followed here can be boiled down as, grouping all the close sentences together based on their meaning and then picking one sentence from each group to minimize redundancy and maximize sentence coverage. Most of the extractive summarization methods involve scoring the sentences based on a sentence scoring metric (generally a version of TF-IDF) and then picking the best scoring sentences to generate the summaries~\cite{Akter-2017-tfidf-3,das-2022-tfidf,sarkar-2012-tfidf,sarkar-2012-tfidf-2}. But our method, firstly, grouped the sentences with similar meaning together and then picked one sentence from each group to generate a summary. This will ensure maximum coverage of topics while also reducing redundancy. But the main challenge to reducing the redundancy is to develop a method that can accurately predict how close the meaning of the two sentences is. In this paper, we propose a method that can more accurately determine sentence similarity. The summarization process followed here involves 4 steps. These are, Pre-processing, Sentence similarity calculation, Clustering and Summary generation. These steps are further discussed in the following subsections.

\subsection{Pre-processing}\label{subsec:pre-processing}
Pre-processing is a standard step in NLP that transforms the raw human language inputs into a format that can be used by a computer algorithm. Here the input document is transformed into a list of sets of vectors where each word is represented with a vector, each sentence as a set of vectors and the whole document as a list of said sets. The Pre-processing involves 3 steps. These are Tokenization, Stop Word Removal, and Word Embedding. A very common step in Pre-processing, Word Stemming, isn't used in here as the word embedding dataset works best for the whole word instead of the stemmed word. These steps are further discussed below.\\

Tokenization is the step of dividing an input document into sentences and words in a usable format. These sentences are then further divided into words and are put together in a list. Stop words, such as prepositions and conjunctions, add sentence fluidity but donâ€™t carry significant meaning. For Bengali, a commonly used dataset\footnote{\textit{https://www.ranks.nl/stopwords/bengali}} of 363 Bengali stop words used to remove the stop words on a matching basis. Word Embedding is a step that replaces the words in a sentence with a corresponding vector in a vector space such that semantically similar words are placed closer together. To achieve this step, we used a dataset with 1.47 million Bengali words produced by \citeauthor{grave-etal-2018-fasttext} \cite{grave-etal-2018-fasttext} crawling Wikipedia and other online resources. Finally, each word that is present in the tokenized and filtered list is replaced with their corresponding vectors, and the words that aren't found are ignored and considered to be too rare to be relevant.

\subsection{Sentence Similarity Calculation}\label{subsec:sentence-similarity-calculation}
Sentence-similarity is a key aspect for building a graphical representation of the sentences in the input document that can be used to group sentences together. This grouping is done using affinity matrix, where each sentence is represented as node and edge as the similarity of the sentences. So, to achieve accurate clustering, a novel similarity calculation technique using individual word distance and Gaussian similarity has been proposed here.\\

The similarity between individual words in a pair of sentences has been considered. For this, firstly, the Most Similar Word Distance ($D_{msw}$) have to be calculated as shown in Equation~\ref{eq:msd}. $D_{msw}$ Denotes the distance between a word vector and its closest counterpart from the other sentence.

\begin{equation}\label{eq:msd}
    D_{msw}(x,Y) = min(\{d(x,y_i) : y_i \in Y \})
\end{equation}

Here, for every word vector $x$ in a sentence $X$, we find the closest word vector $y_i$ from the sentence $Y$. This distance is marked as $D_{msw}(x,Y)$. $d(x,y_i)$ denotes Euclidean distance between two vector $x$ and $y_i$. We collect all the $D_{msw}(x,Y)$ for each word in each sentence and put them together in a list like shown in Equation~\ref{eq:mswdset}.

\begin{equation}
    D = \{D_{msw}(x,Y) : x \in X\} \cup \{D_{msw}(y,X) : y \in Y\}
    \label{eq:mswdset}
\end{equation}

The word similarity is calculated using Gaussian similarity for each of the element of $D$. The equation for word similarity is shown in Equation~\ref{eq:wsim}.

\begin{equation}\label{eq:wsim}
    W_{sim} = \{ exp\left(\frac{-D_i^2}{2\sigma^2}\right) : D_i \in D\}
\end{equation}

Here, $\sigma$ denotes the standard deviation which represents how much noise sensitive the similarity should be. A smaller $\sigma$ represents a highly noise sensitive similarity. This is used as a control variable to further fine tune the algorithm. The similarity between the two sentence, $Sim(X,Y)$, is calculated by taking the Geometric mean of all the word similarities from both sentences. Taking $D_{msw}$ from both sentences makes the $Sim(X,Y)$ value symmetrical as this would result in $Sim(X,Y) = Sim(Y,X)$. The process of getting $Sim(X,Y)$ is explained in the following Equation~\ref{eq:sent_sim}

\begin{equation}\label{eq:sent_sim}
    \begin{split}
        Sim(X,Y)
        &=  \left(
                \prod_{i=1}^nW_{Sim_i}
            \right)^{\frac{1}{n}}\\
        &=  \left(
                e^{\frac{-D_1^2}{2\sigma^2}}\cdot
                e^{\frac{-D_2^2}{2\sigma^2}}\cdot
                    \ldots \cdot
                e^{\frac{-D_n^2}{2\sigma^2}}
            \right)^\frac{1}{n}\\
        &=  exp\left(
                -\frac{D_1^2+D_2^2+\ldots+D_n^2}{2n\sigma^2}
            \right)\\
        &=  exp\left(
                -\frac{\sum_{i=1}^nD_i^2}{2n\sigma^2}
            \right)
    \end{split}
\end{equation}

Here, by taking geometric means of the similarity between the closest words together in two sentences, an effective word to word comparison has been created between those sentences. This reduces any misleading distance that would have come from the word averaging method due to the tendency towards center. An example of this solution is depicted through Figure~\ref{fig:msd}. Here, a more representative word association can be seen for both scenarios from Figure~\ref{fig:sarkar-problem}. Red and Blue dots in the figure represent two sets of word vectors in a sentence pair. Black-dashed lines show the Most Similar Word Distance, $(D_{msw}(x,Y))$, for a word vector $x$ and the other sentence $Y$. The arrowheads point from $x$. The Figure~\ref{fig:msd}(a) shows the $D_{msw}$ for Scenario A in Figure~\ref{fig:sarkar-problem}(a). The Figure~\ref{fig:msd}(b) Shows the $D_{msw}$ for Scenario B in Figure~\ref{fig:sarkar-problem}(b). We can see that the sentences with closer words have smaller $D_{msw}$s and would have smaller geometric mean than the sentences with words that are farther apart. So the problem caused by the averaging method has been mitigated here.\\

The standard deviation $\sigma$ in the Equation \ref{eq:sent_sim} was fine-tuned to be $5\times10^{-11}$ where it gave the best results (Figure~\ref{fig:sigma-fine-tuning}). The process of building the affinity matrix, $A$, is described in the Algorithm~\ref{alg:similarity}. Here, line 9--13 and 19--23 are the process of getting $D_{msw}$. Line 7--26 describes the process of getting $\sum^n_{i=1}D_i^2$. Line 4--29 describes the process of getting $Sim(Sentence_i,Sentence_j)$. Line 1--31 describes the process of getting the affinity matrix, $A$.

\begin{figure}
    \centering
    \input{figs/0301_msd}
    \caption{Process of obtaining $D_{msw}$}
    \label{fig:msd}
\end{figure}
\begin{algorithm} \caption{Sentence Similarity Calculation} \label{alg:similarity}
\begin{algorithmic}[1]
    \State $n \gets$ length(WordVectorList)
    \State $A \gets \{ \{0\} \times n \} \times n$
    \For{each sentence$_i$ in WordVectorList}
        \State $D_{\text{Square}} \gets 0$
        \State count $\gets 0$
        \For{each sentence$_j$ in WordVectorList}
            \For{each word$_i$ in sentence$_i$}
                \State $D_{\text{msw}} \gets \infty$
                \For{each word$_j$ in sentence$_j$}
                    \If{Distance(word$_i$, word$_j$) $< D_{\text{msw}}$}
                        \State $D_{\text{msw}} \gets$ Distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{\text{Square}} \gets D_{\text{Square}} + D_{\text{msw}}^2$
                \State count++
            \EndFor
            \For{each word$_j$ in sentence$_j$}
                \State $D_{\text{msw}} \gets \infty$
                \For{each word$_i$ in sentence$_i$}
                    \If{Distance(word$_i$, word$_j$) $< D_{\text{msw}}$}
                        \State $D_{\text{msw}} \gets$ Distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{\text{Square}} \gets D_{\text{Square}} + D_{\text{msw}}^2$
                \State count++
            \EndFor
            \State similarity $\gets \exp \left( \frac{- D_{\text{Square}}}{2 \times \text{count} \times \sigma^2} \right)$
            \State $A[i][j] \gets A[j][i] \gets$ similarity
        \EndFor
    \EndFor
    \State \textbf{Return} $A$
\end{algorithmic}
\end{algorithm}

\subsection{Clustering}\label{subsec:clustering}
The clustering is the most integral part of this summarization technique, aiming to group all the sentences with similar meanings together. Here, spectral clustering is used to cluster the sentences using sentence similarity calculated in the step above. Spectral clustering was chosen here because \citeauthor{roychowdhury-etal-2022-spectral-base} \cite{roychowdhury-etal-2022-spectral-base} found it to be better performing than DBSCAN method. The spectral clustering steps were followed according to the tutorial given by \cite{vonLuxburg-2007-spectral-tutorial}. \\

To perform spectral clustering on a data, firstly, an affinity matrix is required that shows the weight of edges between the vertexes in the graph. Here the affinity $A$ is prepared using the following Equation~\ref{eq:affinity}.

\begin{equation}\label{eq:affinity}
    A_{ij}=A_{ji}=Sim(S_i,S_j)
\end{equation}

Here, $S_i, S_j$ are sentences from the input document. The affinity matrix, $A$, is used in the spectral clustering which is implemented using SciKit-learn library~\cite{Pedregosa-2011-scikit-learn} of python. It is also necessary to provide the number of clusters to achieve. The number of clusters is fixed at $k=ceiling\left(\frac{N}{5}\right)$ due to it being a reasonable size to contain all necessary sentences as well as being short enough to be an effective summary.

\subsection{Summary Generation}\label{subsec:summary-generation}
Summarized text is the collection of selected sentences from different clusters. After clustering, we pick one sentence from each cluster. The sentences inside a cluster are ranked among themselves using TF-IDF techniques. From each cluster, the sentence with the most TF-IDF score is selected. We then rearranged these picked sentences are in their order of appearance to retain the normal flow of information in the input. These sentences are then concatenated together to produce the final output summary. The clustering and summary generation process is shown in Algorithm~\ref{alg:summary}. After clustering in the lines 1 and 2, We ranked the sentences in the lines 3--8. The best sentence indexes are picked in the lines 9--11. The summary is generated in the lines 12--16.

\begin{algorithm} \caption{Summary Generation} \label{alg:summary}
\begin{algorithmic}[1]
    \State $k \gets \lceil$ length($A$) / 5 $\rceil$
    \State clusters $\gets$ spectral\_clustering(adjacency = $A$, $k$)
    \State indexes $\gets \{\}$
    \For{each cluster$_i$ in clusters}
        \State TFIDF $\gets \{\}$
        \For{each index in cluster$_i$}
            \State TFIDF.append(tfidf\_sum(sentences[index]))
        \EndFor
        \State indexes.append(indexof(max(TFIDF)))
    \EndFor
    \State sort(indexes)
    \State $S \gets `` "$
    \For{each $i$ in indexes}
        \State $S \gets S +$ sentences[$i$]
    \EndFor
    \State \textbf{Return} $S$
\end{algorithmic}
\end{algorithm}

