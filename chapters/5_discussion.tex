The results presented in the previous sections highlight the effectiveness of the proposed WSbSC model for extractive text summarization in Bengali, as well as its adaptability to other low-resource languages. This section analyses the comparative results, the strengths and limitations of the proposed method, and potential areas for further research.\\

As evidenced by the results shown in Table~\ref{tab:result_comparison-1} and Figure~\ref{fig:radarchart}, the WSbSC model consistently outperforms other graph-based extractive text summarization models, namely BenSumm \cite{das-2022-tfidf}, LexRank \cite{Erkan-lexRank-2004}, and SASbSC \cite{roychowdhury-etal-2022-spectral-base}, across multiple datasets (Self-curated, Kaggle, BNLPC, GitHub) of varying sizes and source. The proposed model shows performance improvement in three ROUGE metrics (Rouge-1, Rouge-2, Rouge-LCS). This performance improvement can largely be attributed to the novel approach of calculating sentence similarity due to being one of the key change from the SASbSC method \cite{roychowdhury-etal-2022-spectral-base}. While calculating sentence similarity, taking the geometric mean of individual similarity between word pairs overcomes the lack of local word correspondence faced by the averaging vector method. The Gaussian similarity-based approach used to calculate the word similarities provides a novel and precise method for capturing the semantic relationships between sentences to further contribute in the performance improvement. Another reason for performance improvement is the usage of spectral clustering which is very effective in capturing irregular cluster shapes.\\

To calculate the similarity, we developed a novel method that can calculate the similarity between two set of vectors and is relevant in the context of languages. Our proposed strategy is more suited for this task than other strategies that can compare two sets of vectors such as Earth Movers Distance (EMD) \cite{Rubner-19998-emd}, Hausdorff Distance \cite{hausdorff-1914-hausdorff-distance}, Procrustes Analysis \cite{Gower-1975-procrustes-distance} due to it focusing on the context of language more. EMD \cite{Rubner-19998-emd} tries to find the lowest amount of ``work'' needed to transform one set into the other one to calculate similarity. It considers adding a new vector to a set, removing a vector from a set, scaling a set, moving a vector in the set, rotating the set etc.\ as ``work''. This process is very computationally expensive as hundreds of thousands separate possibilities have to be checked for each vector in each set. EMD also focuses on scaling and rotating, which are not relevant in word vector space as rotating a sentence does not hold any semantic meaning. Another method, Hausdorff distance \cite{hausdorff-1914-hausdorff-distance} takes the worst case scenario and calculates the highest distance between two vectors each from a set. This method needs the same amount of calculation as the proposed method but it is easily influenced by outliers due to only considering the worst case scenario. It also does not have any local vector correspondence and wasn't used because words tend to spread out over the whole word space and it would suffer from the same problem as the averaging method. Procrustes Analysis \cite{Gower-1975-procrustes-distance} tries to find the lowest amount of misalignment after scaling and rotating the two sets. But both scaling and rotating are irrelevant in the context of word vectors.\\

On the other hand, the proposed method focuses on local vector correspondence between two sets which is more important for words. The Gaussian similarity function captures the proximity of points smoothly, providing a continuous value for similarity between two words in a normalized way. Gaussian similarity is also robust against small outliers due to being a soft similarity measure. Taking geometric mean also helps smooth over similarity values for outlier words.\\

One of the key strengths of this proposed method is the reduction of redundancy, which is a common issue in extractive summarization methods. By grouping sentences with similar meanings and selecting a representative sentence from each group, the model ensures that the summary covers a broad range of topics without repeating itself. The use of spectral clustering in the model is also well-suited for the clustering task because spectral method does not assume a specific cluster shape and can infer the number of clusters using the eigen gap method. Our proposed model also has an improved sentence similarity calculation technique. Using the geometric mean of individual word similarities offers a more precise measure of how closely two sentences are related. This is a significant improvement over other traditional methods that rely on word averaging, which often dilute the semantic meaning of a sentence. Another key strength is the scalability across languages. WSbSC can be easily adapted to other languages due to it requiring very few language-specific resources. This scalability is demonstrated in the experiments with Hindi, Marathi, and Turkish languages (Table~\ref{tab:other_language}). This generalizability makes the model highly versatile and valuable for extractive summarization in low-resource languages. Despite differences in language structure, the model’s core methodology remained effective, yielding results that were consistent with the Bengali dataset evaluations. This underscores the potential of WSbSC as a generalizable approach for extractive summarization across different languages, if appropriate pre-processing tools and word embedding datasets are available.\\

Despite its advantages, the WSbSC model does face some challenges. The model heavily relies on pre-trained word embeddings, which may not always capture the full details of certain domains or newly coined terms. The FastText \cite{grave-etal-2018-fasttext} dataset used here is heavily reliant on wikipedia for training which could introduce some small unforeseen biases. In cases where the word embeddings do not have some words of a given document, the model’s performance could degrade as it leaves those words out of the calculation process. The model also does not take into account the order in which words appear in a sentence or when they form special noun or verb groups. So it can be a little naive in some highly specialized fields.\\

The proposed WSbSC model represents a significant advancement in Bengali extractive text summarization. Its ability to accurately capture sentence similarity, reduce redundancy, maximize coverage and generalize across languages makes it a valuable tool for summarizing text in low-resource languages. While there are still challenges to be addressed, the results of this study demonstrate the robustness and adaptability of the WSbSC model, offering a promising direction for future research in multilingual extractive summarization.
