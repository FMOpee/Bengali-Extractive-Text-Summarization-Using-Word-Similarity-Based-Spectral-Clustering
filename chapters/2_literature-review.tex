Text summarization has been an important necessity for textual data consumption for a long time because of its ability to compress a given input text into a shortened version without losing any key information. For this reason, automating the text summarization process has been a research problem for NLP. Thus researchers attempted automatic text summarization for a long time too. At first, indexing-based text summarization methods were attempted such as the work by \citeauthor{Baxendale_1958_firstsummarization} \cite{Baxendale_1958_firstsummarization}. This method scored sentences based on the presence of indexing terms in the sentence and picked the sentences with the best score. But this type of method failed to capture the topic and essence of the input text as we wouldn't have the topic keywords of an unforeseen input document. To solve this issue, text summarization with statistical methods like TF-IDF became very popular due to its ability to capture the important topic words of a document in an unsupervised manner. \citeauthor{edmundson_1969_earlysum} \cite{edmundson_1969_earlysum} proposed a method which can focus on the central topic of a document by using the TF-IDF measure. It uses two metrics, Term Frequency (how many times a term appears in the input) and Inverse Document Frequency (inverse of how many documents the term appears in a large text corpus) to calculate the importance of a term in a document. Using TF-IDF helps to identify the words that are common in the input text but not as common in the language in general and thus classifying them as the central topic of the document. But this method is also error-prone due to its consideration of every word as a unique isolated term without any semantic relation with other words. This leads to the method often missing some central topic if it gets divided into too many synonyms.\\ 

The problems faced by topic-based summarization methods were alleviated by graph-based extractive text summarization methods which brought new modern breakthroughs. Graph-based methods like LexRank \cite{Erkan-lexRank-2004} and TextRank \cite{mihalcea-2004-textrank} were able to capture the relationship between sentences more accurately due to use of sentence similarity graph in their process. LexRank \cite{Erkan-lexRank-2004} calculates the similarity graph by using cosine similarity of bag-of-words vectors between two sentences from the input. The most important sentences from the graph are classified using the PageRank \cite{page-PageRank-1999} algorithm on the graph. PageRank ranks those sentences higher who are more similar with other high ranked sentences. Another graph-based method, TextRank \cite{mihalcea-2004-textrank} also uses a similar approach while building the similarity graph. In the graph for every sentence, TextRank distributed the score of that sentence to its neighbours using a random walk. This process is repeated over and over until the scores converge. Then the method picks the sentences with the best scores as the summary. Although graph-based methods such as LexRank and TextRank models are Ground-breaking compared to their time, they still lacked fundamental understanding of the words involved in a sentence due to not using any vector representation of the semantic relationship between the words involved.\\

To solve the problem of representing semantic relationship, a mathematical abstraction called Word Vector Embedding was conceptualized by the seminal work of \citeauthor{salton-1975-word-vector} \cite{salton-1975-word-vector}. Word Vector Embedding uses a word vector space as a mathematical abstraction of a lexicon where the closer two words are semantically, the closer they are in the vector space. Using word vector for graph based text summarization has only been started to be attempted recently \cite{Jain-2017-word-vector-embedding-summary} due to the lack of fully developed word embedding datasets.\\

Although text summarization have been a forefront of NLP research, text summarization research in Bengali is a more recent development than in other high resource languages. So, a lot of sophisticated approaches from other languages haven't been attempted at Bengali yet. Earlier bengali extractive methods have been focused on some derivative of TF-IDF based text summarization such as the methods developed by \citeauthor{chowdhury-etal-2021-tfidf-clustering} \cite{chowdhury-etal-2021-tfidf-clustering}, \citeauthor{das-2022-tfidf} \cite{das-2022-tfidf}, \citeauthor{sarkar-2012-tfidf} \cite{sarkar-2012-tfidf} etc. \citeauthor{sarkar-2012-tfidf} \cite{sarkar-2012-tfidf} used a simple TF-IDF score of each sentence to rank them and pick the best sentences to generate the output summary. \citeauthor{das-2022-tfidf} \cite{das-2022-tfidf} used weighted TF-IDF along with some other sentence features like sentence position to rank the sentences. \citeauthor{chowdhury-etal-2021-tfidf-clustering} \cite{chowdhury-etal-2021-tfidf-clustering} however, used TF-IDF matrix of a document to build a graph and perform hierarchical clustering to group sentences together and pick one sentence from each group. One shortcoming of this method is that TF-IDF matrix is not semantically equivalent to the actual sentences due to the fundamental issues with TF-IDF. So, the TF-IDF doesn't perfectly represent the semantic relationship between the sentences in the generated graph. Using word vector embedding for Bengali has solved this problem of semantic representation. Word embedding datasets such as FastText \cite{grave-etal-2018-fasttext} dataset\footnote{\textit{https://fasttext.cc/docs/en/crawl-vectors.html}} with word vector embeddings in 157 languages, including Bengali. Using this dataset, SASbSC \cite{roychowdhury-etal-2022-spectral-base} proposed a model where they replaced all the words from the input with their respective vector, then averaged the word vectors in a sentence to get a vector representation for the sentence. The sentence average vectors are then used to get the similarity between two sentences using the Gaussian similarity function to build an affinity matrix. This affinity matrix is used to cluster the sentences using spectral clustering to group sentences into distinct topics. The summary is generated after picking one sentence from each cluster to reduce redundancy and increase coverage.\\

\begin{figure}
    \centering
    \input{figs/0201_Sarkar_problem}
    \caption{A scenario where sentence averaging method fails. Dots in the figure represent word vectors and are grouped by colours if they are from the same sentence. (a) and (c) shows a scenario where the distance between sentence average vectors are larger than (b) and (d) despite the word vectors being more closely related.}
    \label{fig:sarkar-problem}
\end{figure}

But the sentence average method suffers critically due to its inability in capturing accurate relationship between sentences. This happens due to words in a sentence generally not having similar meaning with each other, instead they express different parts of one whole meaning of a sentence. This makes the words more complementary instead of being similar leading to word vectors being scattered throughout the word vector space. This characteristics makes the sentence average vectors always tending towards the centre and not representing the semantic similarity accurately. An example of this happening is shown in Figure \ref{fig:sarkar-problem} where the distance between the sentence average vectors are being misleading. In the figure, scenario (a) shows two sentence with word vectors very closely corresponding with each other. On the other hand, scenario (b) shows two sentences without any significant word correspondence. The distance between the sentence average vectors  from (a) and (b) are shown in figure (c) and (d) respectively. We can see that the scenario (a) and (c) have a larger distance between sentence average vectors than scenario (b) and (d) despite having more word correspondence. This larger distance makes the Gaussian similarity between the sentences lower due to the inverse exponential nature of the function. The lower similarity leads to the graphical representation being less accurate and thus failing to capture the true semantic relationship within the sentences. This shortcoming of the method has been one of the key motivations for this research.