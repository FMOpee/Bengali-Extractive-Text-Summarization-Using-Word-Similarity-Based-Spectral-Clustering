Text Summarization is the process of shortening a larger text without losing any key information to increase the readability and information density of the input text and save time for the reader. But manually summarizing very large texts is a counter-productive task due to it being more time consuming and tedious. So, developing an Automatic Text Summarization (ATS) method that can summarize larger input texts reliably is really necessary to alleviate this manual labour \cite{Widyassari-2022-rev-ats-tech-met}. ATS is a major area of rapidly progressing Natural Language Processing (NLP) research due to the importance of summarization increasing significantly. This increase can be attributed to the information overflow in the digital era due to the trend of exponential growth of textual data in last few years \cite{2015-Forbes-80-created-last-2-years}. Using ATS to summarize textual data is thus becoming very important in various fields such as news articles, legal documents, health reports, research papers, social media contents etc. ATS helps the reader to quickly and efficiently get the essential information without needing to read through large amounts of texts by reducing the volume of text significantly \cite{wafaa-2021-summary-comprehensive-review}. So ATS is being utilized in various fields, from automatic news summarization, content filtering, and recommendation systems to assisting legal professionals in going through long documents and researchers in reviewing academic papers by condensing vast amount of informations. It can also play a critical role in personal assistants and chatbots, providing condensed information to users quickly and efficiently \cite{tas-2017-rev-text-sum-2}.\\

There are two main types of ATS: extractive and abstractive \cite{tas-2017-rev-text-sum-2}. Extractive summarization, which is the focus of this paper, works by selecting a subset from the source document, maintaining the original wording and sentence structure \cite{moratanch-2017-extractive-review}. In contrast, abstractive summarization involves synthesising new text that reflects on and has information from the input document but does not copy from it, similar to how a human summarizes a text \cite{Moratanch-2016-abstractive-rev}. Both of the method has their own advantage. The abstractive summarization can simulate the human language pattern very well thus increasing the natural flow and readability of the summary. But the extractive method requires much less computation than the abstractive method while also containing more key informations from the input \cite{gupta-2010-extractive-rev}.\\ 

The key approach to extractive summarization is implementing a sentence selection method to classify which sentences belong in the summary. For this approach, previously various simplistic ranking based methods were used to rank the sentences and identify the best sentences as the summary. These ranking methods used indexing \cite{Baxendale_1958_firstsummarization}, statistical \cite{edmundson_1969_earlysum} or Term Frequency-Inverse Document Frequency (TF-IDF) \cite{das-2022-tfidf,sarkar-2012-tfidf-2,sarkar-2012-tfidf} based techniques to score the sentences and  select the best scoring ones. But these methods fail to capture the semantic relationships between sentences of the input due to being simplistic in nature. To capture the semantic relationships between sentences, graph based extractive methods are more effective due to them using a sentence similarity graph in their workflow \cite{wafaa-2021-summary-comprehensive-review}. Graph based methods represent the sentences from the input document as nodes of a graph, and the semantic similarity between two sentences as the edge between the nodes \cite{moratanch-2017-extractive-review}. Popular graph based algorithms like LexRank \cite{Erkan-lexRank-2004} and TextRank \cite{mihalcea-2004-textrank} build graphs based on cosine similarity between the bag-of-word vectors to build this graph.LexRank uses PageRank \cite{page-PageRank-1999} method to score the sentences from the graph while TextRank uses random walk to determine which sentences are the most important to be in the summary. Graph-based methods like TextRank and LexRank offer a robust way to capture sentence importance and relationship, ensuring that the extracted summary covers the key information while minimizing redundancy~\cite{wafaa-2021-summary-comprehensive-review}.\\  

Clustering-based approaches are a subset of graph-based approach to extractive text summarization. Here, sentences are grouped into clusters based on their semantic similarity to divide the document into topics, and one representative sentence from each cluster is chosen to form the summary \cite{Mohan-2022-topic-modeling-rev-clustering}. Clustering reduces redundancy by ensuring that similar sentences are grouped together and that only the most representative sentence is selected. This method is effective in summarization of documents with multiple topics or subtopics, as it allows the summary to touch on each area without being repetitive. An example of this method can be seen with COSUM \cite{alguliyev-2019-cosum} where the summarization is achieved using k-means clustering on the sentences and picking the most salient sentence from each cluster to compile in the final summary.\\

Despite the advancements of ATS in other languages, it remains an under-researched topic For Bengali due to Bengali being a low-resource language. Early attempts at Bengali text summarization relied on traditional methods like TF-IDF scoring \cite{Akter-2017-tfidf-3, das-2022-tfidf, sarkar-2012-tfidf, sarkar-2012-tfidf-2} to score TF-IDF value for a sentence and use them to select the best scoring sentences to form the summary. These TF-IDF based approaches, while simple, faced challenges in capturing the true meaning of sentences, as they treated words as isolated terms so synonyms would get treated as completely different terms \cite{tas-2017-rev-text-sum-2}. To solve this problem, graph-based methods were introduced in Bengali. Although graph-based methods improved summarization quality by incorporating sentence similarity, they were still limited by the quality of the embeddings used for the Bengali language. With the advent of word embedding models like FastText \cite{grave-etal-2018-fasttext}, it became possible to represent words in a vector space model, thus enabling more accurate sentence similarity calculations. However, existing models that use word embeddings, such as Sentence Average Similarity-based Spectral Clustering (SASbSC)  method \cite{roychowdhury-etal-2022-spectral-base}, encountered issues with sentence-similarity calculation  when averaging word vectors to represent the meaning of a sentence with a vector. This method failed in most similarity calculation cases because words in a sentence are often complementary to each other rather than being similar, leading to inaccurate sentence representations when averaging their vectors. As a result, word-to-word relationships between sentences were lost, reducing the effectiveness of the method.\\

In this paper, we propose a new approach to address these challenges. Our method improves upon previous attempts~\cite{roychowdhury-etal-2022-spectral-base} by focusing on the individual similarity between words in sentences rather than averaging word vectors. Here, we followed a number of steps to get the similarity between two sentences. At first, for each of a sentence, we picked the closest word vector from the other sentence. Then, we took the Gaussian Similarity for these two vectors. We did this  for every word of both sentence. We take the geometric mean of these similarities to get the sentence similarity between the two sentences. By applying Gaussian similarity to the Most Similar Word Distance ($D_{msw}$) values, we build an affinity matrix that better reflects sentence closeness. Then, we applied spectral clustering on this matrix to group similar sentences together and used TF-IDF to select the most representative sentences from each cluster. This approach reduces redundancy and improves the quality of the summary by selecting sentences that are not only relevant but also diverse. This method works well for Bengali on four diverse datasets (Figure~\ref{fig:radarchart}). It consistently outperforms other graph-based methods like BenSumm~\cite{chowdhury-etal-2021-tfidf-clustering}, SASbSC~\cite{roychowdhury-etal-2022-spectral-base}, LexRank~\cite{Erkan-lexRank-2004}. It also performs similarly well in other low resource languages such as Hindi, Marathi and Turkish (Table~\ref{tab:other_language}). These are the only other low resource languages where we found reliable evaluation datasets and tested our model on them. The search process was not exhaustive due to our language barrier. The whole process of summarization is shown in the Process Flow Diagram (Figure~\ref{fig:process-flow-diagram})\\

\begin{figure}
    \centering
    \input{figs/0101_process_flow_diagram}
    \caption{Process Flow Diagram}
    \label{fig:process-flow-diagram}
\end{figure}

The main contributions of this paper are:
(I) Proposed a new way to calculate similarity between two sentences.
(II) Contributes a novel methodology for extractive text summarization for the Bengali language; by improving sentence similarity calculations and enhancing clustering techniques.
(III) It offers a generalizable solution for creating less redundant and information rich summaries across languages.
(IV) It provides a publicly available high quality dataset of 500 human generated summaries.\\

The rest of the paper is organized as follows: The Literature review and Methodology are described in section 2 and 3 respectively. The section 4 illustrates the findings of this work. The section 5 discusses the findings of the paper in more depth, and section 6 concludes the paper.