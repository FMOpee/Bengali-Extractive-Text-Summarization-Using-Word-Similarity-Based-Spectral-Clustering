Text Summarization is the process of shortening a larger text without losing any key information to increase the readability and save time for the reader. But manually summarizing very large texts is a counter-productive task due to it being more time consuming and tedious. So, developing an Automatic Text Summarization (ATS) method that can summarize larger texts reliably is really necessary to alleviate this manual labour \cite{Widyassari-2022-rev-ats-tech-met}. 
%ATS is a major area of rapidly progressing Natural Language Processing (NLP) research due to the importance of summarization increasing significantly. This increase can be attributed to the information overflow in the digital era due to the trend of exponential growth of textual data in last few years \cite{2015-Forbes-80-created-last-2-years}. 
Using ATS to summarize textual data is thus becoming very important in various fields such as news articles, legal documents, health reports, research papers, social media contents etc. ATS helps the reader to quickly and efficiently get the essential information without needing to read through large amounts of texts \cite{wafaa-2021-summary-comprehensive-review}. So, ATS is being utilized in various fields, from automatic news summarization, content filtering, and recommendation systems to assisting legal professionals in going through long documents. And researchers in reviewing academic papers by condensing vast amount of informations. It can also play a critical role in personal assistants and chatbots, providing condensed information to users quickly and efficiently \cite{tas-2017-rev-text-sum-2}.\\

There are two main types of ATS: extractive and abstractive \cite{tas-2017-rev-text-sum-2}. Extractive summarization, which is the focus of this paper, works by selecting a subset from the source document, maintaining the original wording and sentence structure \cite{moratanch-2017-extractive-review}. In contrast, abstractive summarization involves synthesising new text that reflects information from the input document but does not copy from it, similar to how a human summarizes a text \cite{Moratanch-2016-abstractive-rev}. Both of the method has their own advantage. The abstractive summarization can simulate the human language pattern very well thus increasing the natural flow and readability of the summary. But the extractive method requires much less computation than the abstractive method while also containing more key informations from the input \cite{gupta-2010-extractive-rev}.\\ 

The key approach to extractive summarization is implementing a sentence selection method to classify which sentences will belong in the summary. For this approach, previously various simplistic ranking based methods were used to rank the sentences and identify the best sentences as the summary. These ranking methods used indexing \cite{Baxendale_1958_firstsummarization}, statistical \cite{edmundson_1969_earlysum} or Term Frequency-Inverse Document Frequency (TF-IDF) \cite{das-2022-tfidf,sarkar-2012-tfidf-2,sarkar-2012-tfidf} based techniques to score the sentences and  select the best scoring ones. But these methods fail to capture the semantic relationships between sentences of the input due to being simplistic in nature. To capture the semantic relationships between sentences, graph based extractive methods are more effective due to them using a sentence similarity graph in their workflow \cite{wafaa-2021-summary-comprehensive-review}. Graph based methods represent the sentences from the input document as nodes of a graph, and the semantic similarity between two sentences as the edge between the nodes \cite{moratanch-2017-extractive-review}. Popular graph based algorithms like LexRank \cite{Erkan-lexRank-2004} and TextRank \cite{mihalcea-2004-textrank} build graphs based on cosine similarity between the bag-of-word vectors to build this graph.LexRank uses PageRank \cite{page-PageRank-1999} method to score the sentences from the graph while TextRank uses random walk to determine which sentences are the most important to be in the summary. Graph-based methods like TextRank and LexRank offer a robust way to capture sentence importance and relationship, ensuring that the extracted summary covers the key information while minimizing redundancy~\cite{wafaa-2021-summary-comprehensive-review}.\\  

Clustering-based approaches are a subset of graph-based approach to extractive text summarization. Here, sentences are grouped into clusters based on their semantic similarity to divide the document into topics, and one representative sentence from each cluster is chosen to form the summary \cite{Mohan-2022-topic-modeling-rev-clustering}. Clustering reduces redundancy by ensuring that similar sentences are grouped together and that only the most representative sentence is selected. This method is effective in summarization of documents with multiple topics or subtopics, as it allows the summary to touch on each area without being repetitive. An example of this method can be seen with COSUM \cite{alguliyev-2019-cosum} where the summarization is achieved using k-means clustering on the sentences and picking the most salient sentence from each cluster to compile in the final summary.\\

Despite the advancements of ATS in other languages, it remains an under-researched topic For Bengali due to Bengali being a low-resource language. Early attempts at Bengali text summarization relied on traditional methods like TF-IDF scoring to score TF-IDF value for a sentence and use them to select the best scoring sentences to form the summary \cite{Akter-2017-tfidf-3, das-2022-tfidf, sarkar-2012-tfidf, sarkar-2012-tfidf-2}. These TF-IDF based approaches, while simple, faced challenges in capturing the true meaning of sentences, as they treated words as isolated terms so synonyms would get treated as completely different terms \cite{tas-2017-rev-text-sum-2}. To solve this problem, graph-based methods were introduced in Bengali which although improved summarization quality by incorporating sentence similarity, they were still limited by the quality of word embeddings used for the Bengali language. With the advent of word embedding models like FastText \cite{grave-etal-2018-fasttext}, it became possible to represent words in a vector space model, thus enabling more accurate sentence similarity calculations. However, existing models that use word embeddings, such as Sentence Average Similarity-based Spectral Clustering (SASbSC) method \cite{roychowdhury-etal-2022-spectral-base}, encountered issues with sentence-similarity calculation when averaging word vectors to represent the meaning of a sentence with a vector. This method failed in most similarity calculation cases because words in a sentence are often complementary to each other rather than being similar, leading to inaccurate sentence representations after averaging these different word vectors. As a result, word-to-word relationships between sentences get lost, reducing the effectiveness of the method.\\

In this paper, we propose a new clustering-based text summarization approach to address the challenge of calculating sentence similarity accurately. Our method improves upon previous attempts at graph-based summarization methods \cite{chowdhury-etal-2021-tfidf-clustering, roychowdhury-etal-2022-spectral-base} by focusing on the individual similarity between word pairs in sentences rather than averaging word vectors. We showed that the use of this novel approach greatly improved the accuracy, coverage and reliability of the output summaries due to having a deeper understanding of the semantic similarity between sentences. To implement this method of calculating sentence similarity, we used the geometric mean of individual word similarities because of the robustness of geometric mean against outliers. The individual word similarities were achieved using Gaussian kernel function on a pair of corresponding word vector from each sentence because Gaussian kernel provides a normalized smooth similarity measure between two vectors. The corresponding word pairs are selected by finding the word vector with the smallest Euclidean distance from the opposing sentence. This distance to the most similar word is defined as the Most Similar Word Distance ($D_{msw}$) in this article. Following these steps described above, we can get the semantic similarity between two sentences which can be used to build an affinity matrix to graphically represent the relationship between the sentences of the input. This graph can be clustered into groups to divide the document into distinct topics. We select one sentence from each of these clusters to reduce redundancy and increase topic coverage by selecting sentences which are relevant and diverse. This method consistently outperforms other graph based text summarization methods such as BenSumm \cite{chowdhury-etal-2021-tfidf-clustering}, LexRank \cite{Erkan-lexRank-2004}, SASbSC \cite{roychowdhury-etal-2022-spectral-base} using four evaluation datasets with varying sizes and sources on all three ROUGE metric \cite{lin-2004-rouge} as shown in figure \ref{fig:radarchart} and table \ref{tab:result_comparison-1}. The proposed method also performs similarly well in other low resource languages such as Hindi, Marathi and Turkish due to the language independent nature of the method as shown in the table \ref{tab:other_language}. These three languages are the only low resource languages where we found reliable evaluation datasets and tested our model due to our language barrier. The whole process of summarization is shown in the Process Flow Diagram (Figure~\ref{fig:process-flow-diagram})\\

\begin{figure}
    \centering
    \input{figs/0101_process_flow_diagram}
    \caption{Process Flow Diagram}
    \label{fig:process-flow-diagram}
\end{figure}
The main contributions of this paper are:
(I) Proposed a new way to calculate similarity between two sentences.
(II) Contributes a novel methodology for extractive text summarization for the Bengali language; by improving sentence similarity calculations and enhancing clustering techniques.
(III) It offers a generalizable solution for creating less redundant and information rich summaries across languages.
(IV) It provides a publicly available high quality dataset of 500 human generated summaries.\\

The rest of the paper is organized as follows: The Related works and Methodology are described in section \ref{sec:literature-review} and \ref{sec:methodology} respectively. Section \ref{sec:result} illustrates the result of the performance evaluation for this work. Section \ref{sec:discussion} discusses the findings of the paper in more depth, and section \ref{sec:conclusion} concludes the paper.