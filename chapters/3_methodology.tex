The summarization process followed here can be boiled down as two basic steps, grouping all the close sentences together based on their meaning to minimize redundancy and picking one sentence from each group to maximize sentence coverage. To group semantically similar sentences into clusters, we build a sentence similarity graph and perform spectral clustering on it~\cite{roychowdhury-etal-2022-spectral-base}. The sentence similarity graph is produced using a novel sentence similarity calculation algorithm that uses geometric mean of Gaussian similarity between individual word pairs from the two sentences. The Gaussian similarity is calculated using the vector embedding representations of the words. On the other hand, we used TF-IDF scores of the sentences in a cluster to pick the highest ranked and thus most representative sentence from the cluster~\cite{Akter-2017-tfidf-3,das-2022-tfidf,sarkar-2012-tfidf,sarkar-2012-tfidf-2}. The summarization process followed here involves 4 steps. These are, Pre-processing, Sentence similarity calculation, Clustering and Summary generation. These steps are further discussed in the following subsections. 

\subsection{Preprocessing}\label{subsec:preprocessing}
Preprocessing is the standard process of NLP that transforms raw human language inputs into a format that can be used by a computer algorithm. In this preprocessing step, the input document is transformed into a few set of vectors where each word is represented with a vector, each sentence is represented as a set of vectors and the whole document as a list containing those sets. To achieve this representation, the preprocessing follows three steps. These are tokenization, stop word removal, and word embedding. A very common step in preprocessing, word stemming, isn't used here as the word embedding dataset works best for the whole word instead of the root word. These steps are further discussed below.\\

Tokenization is the step of dividing an input document into sentences and words to transform it into a more usable format. Here, the input document is represented as a list of sentence and the sentences are represented as a list of words. Stop words, such as prepositions and conjunctions, add sentence fluidity but donâ€™t carry significant meaning. Removing these words allows the algorithm to focus on more impactful words. To remove these stop words, we used a list\footnote{\textit{https://www.ranks.nl/stopwords/bengali}} of 363 bengali words. Word Embedding is the process of representing  words as vector in a vector space. In this vector space, semantically similar words are placed closer together so that the similarity relation between words can be expressed in an abstract mathematical way. We used the FastText dataset\footnote{\textit{https://fasttext.cc/docs/en/crawl-vectors.html}}~\cite{grave-etal-2018-fasttext} with 1.47 million Bengali words and their vector representation to achieve this step. Each word from the tokenized and filtered list is replaced with their corresponding vectors. If some words aren't present in the dataset, they are considered too rare and ignored. Following these steps, the input document is transformed into a set of vectors to be used in sentence similarity calculation. 

\subsection{Sentence Similarity Calculation}\label{subsec:sentence-similarity-calculation}
Sentence similarity is the key criteria to build a graphical representation of the relation between the sentences in the input document. This graphical representation is expressed via an affinity matrix to cluster the sentences into groups of semantically similar sentences. The nodes in the affinity matrix represents the sentences of the input and the edges of the matrix represents the similarity between two sentence to graphically represent the input document. Here, we proposed a novel sentence similarity calculation technique using individual Gaussian similarity between close word pairs to construct the affinity matrix. To calculate the sentence similarity between two sentences, we adhere to the following steps.\\

Firstly, for every word of a sentence, we find its closest counterpart from the other sentence to build a word pair. The Euclidean distance between the vector representation of the two words from this pair is defined as the Most Similar Word Distance ($D_{msw}$) to be used in the following steps. The process of calculating the $D_{msw}$ is shown in the equation \ref{eq:msd}. In this equation, for every word vector $x$, in a sentence $X$, we find the Euclidean distance ( $d(x,y_i)$ ) between the word vectors $x$ and $y_i$ where $y_i$ is a word vector from the sentence $Y$. The lowest possible distance in this set of Euclidean distance is the $D_{msw}$. 
\begin{equation}\label{eq:msd}
    D_{msw}(x,Y) = min(\{d(x,y_i) : y_i \in Y \})
\end{equation}
Then, we calculate the $D_{msw}$ for every word of the two sentences $X$ and $Y$ to make the sentence similarity calculation symmetric. This process is shown in the equation \ref{eq:mswdset}. In this equation, $D$, is a set containing all the $D_{msw}$ from both $X$ and $Y$ that would be used in the later steps.
\begin{equation}
    D = \{D_{msw}(x,Y) : x \in X\} \cup \{D_{msw}(y,X) : y \in Y\}
    \label{eq:mswdset}
\end{equation}
After this, the word similarity between the word pairs is calculated to get the degree of word correspondence between the two sentences. Here, the word similarity is calculated using Gaussian kernel function for the elements of the set $D$ because Gaussian kernel functions provides a smooth, flexible and most representative similarity between two vectors \cite{babud-1986-gaussian}. The process of calculating word similarity ($W_{sim}$) is given in the following equation \ref{eq:wsim}. In this equation, for every element $D_i$ in set $D$, we calculate the Gaussian similarity to obtain word similarity. In the formula for Gaussian similarity, $\sigma$ denotes the standard deviation that can be used as a control variable. The standard deviation represents the blurring effect of the kernel function. A lower value for $\sigma$ represents a high noise sensitivity of the function \cite{babud-1986-gaussian}. The value of sigma was fine-tuned to be $5\times10^{-11}$ which gives the similarity measurement. The process of fine-tuning is described in the experimentation section (section \ref{subsubsec:sigma}). 
\begin{equation}\label{eq:wsim}
    W_{sim} = \{ exp\left(\frac{-D_i^2}{2\sigma^2}\right) : D_i \in D\}
\end{equation}
Finally, the sentence similarity between the two sentences $Sim(X,Y)$ is calculated using geometric mean of the word similarities from the above step to construct an affinity matrix. Using geometric mean makes the similarity value less prone to effects of outliers to make the calculation more reliable. This process is shown in the following equation \ref{eq:sent_sim}. In this equation, the geometric mean of each $w_{sim}$ value for the two sentences is simplified in the equation \ref{eq:sent_sim} to make the calculation process more computation friendly. 
\begin{equation}\label{eq:sent_sim}
    \begin{split}
        Sim(X,Y)
        &=  \left(
                \prod_{i=1}^nW_{Sim_i}
            \right)^{\frac{1}{n}}\\
        &=  \left(
                e^{\frac{-D_1^2}{2\sigma^2}}\cdot
                e^{\frac{-D_2^2}{2\sigma^2}}\cdot
                    \ldots \cdot
                e^{\frac{-D_n^2}{2\sigma^2}}
            \right)^\frac{1}{n}\\
        &=  exp\left(
                -\frac{D_1^2+D_2^2+\ldots+D_n^2}{2n\sigma^2}
            \right)\\
        &=  exp\left(
                -\frac{\sum_{i=1}^nD_i^2}{2n\sigma^2}
            \right)
    \end{split}
\end{equation}
By following steps described above, we get a similarity value for two sentence. This value solves the lack of local word correspondence problem faced by the word averaging method based similarity calculation \cite{roychowdhury-etal-2022-spectral-base} by considering local word to word similarity. This correspondence is shown in the figure \ref{fig:msd} to demonstrate the merit of the method described above. Figure \ref{fig:msd} follows up the scenario from the figure \ref{fig:sarkar-problem} to show that the proposed method can solve the local word correspondence problem faced by word averaging method. The figure shows that the scenario \ref{fig:msd}(a) has a set of smaller $D_{msw}$ than the scenario \ref{fig:msd}(b). Having smaller $D_{msw}$ makes the individual word similarities $W_{sim}$ from equation \ref{eq:wsim} larger due to the nature of Gaussian kernel function. These values would result in a higher sentence similarity for the sentences in the scenario \ref{fig:msd}(a) than in the scenario \ref{fig:msd}(b). This solved the problem showed in the figure \ref{fig:sarkar-problem} where the scenario \ref{fig:sarkar-problem}(a) has a larger sentence average distance than \ref{fig:sarkar-problem}(b) resulting in \ref{fig:sarkar-problem}(a) having a smaller sentence similarity than \ref{fig:sarkar-problem}(b).\\

The whole process of sentence similarity calculation is shown in the following algorithm \ref{alg:similarity}. In this algorithm, we calculate an affinity matrix using the input word vector list from the preprocessing step. We took the most similar word distance $D_{msw}$ in line 8--13 and 18--23 for each word (line 7 and 17) of a sentence pair (line 3 and 6). $\sum^n_{i=1}D_i^2$ from equation \ref{eq:sent_sim} is calculated in the lines 14 and 24 to be used in the calculation of sentence similarity (line 27). The similarity is used to construct an affinity matrix $A$ (line 28).
\begin{figure}
    \centering
    \input{figs/0301_msd}
    \caption{Process of obtaining $D_{msw}$}
    \label{fig:msd}
\end{figure}
\begin{algorithm} \caption{Sentence Similarity Calculation} \label{alg:similarity}
\begin{algorithmic}[1]
    \State $l \gets$ length(WordVectorList)
    \State $A \gets [ [ 0 ] * l ] * l$
    \For{each sentence$_i$ in WordVectorList}
        \State $D_{Square} \gets 0$
        \State n $\gets 0$
        \For{each sentence$_j$ in WordVectorList}
            \For{each word$_i$ in sentence$_i$}
                \State $D_{msw} \gets \infty$
                \For{each word$_j$ in sentence$_j$}
                    \If{Distance(word$_i$, word$_j$) $< D_{msw}$}
                        \State $D_{msw} \gets$ distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{Square} \gets D_{Square} + D_{msw}^2$
                \State $n \gets n+1$ 
            \EndFor
            \For{each word$_j$ in sentence$_j$}
                \State $D_{msw} \gets \infty$
                \For{each word$_i$ in sentence$_i$}
                    \If{Distance(word$_i$, word$_j$) $< D_{msw}$}
                        \State $D_{msw} \gets$ distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{Square} \gets D_{Square} + D_{msw}^2$
                \State $n \gets n+1$
            \EndFor
            \State similarity $\gets \exp \left( \frac{- D_{Square}}{2 \times n \times \sigma^2} \right)$
            \State $A[i][j] \gets A[j][i] \gets$ similarity
        \EndFor
    \EndFor
    \State \textbf{Return} $A$
\end{algorithmic}
\end{algorithm}

\subsection{Clustering}\label{subsec:clustering}
The clustering is the most integral part of this summarization technique, aiming to group all the sentences with similar meanings together. Here, spectral clustering is used to cluster the sentences using sentence similarity calculated in the step above. Spectral clustering was chosen here because \citeauthor{roychowdhury-etal-2022-spectral-base} \cite{roychowdhury-etal-2022-spectral-base} found it to be better performing than DBSCAN method. The spectral clustering steps were followed according to the tutorial given by \cite{vonLuxburg-2007-spectral-tutorial}. \\

To perform spectral clustering on a data, firstly, an affinity matrix is required that shows the weight of edges between the vertexes in the graph. Here the affinity $A$ is prepared using the following Equation~\ref{eq:affinity}.

\begin{equation}\label{eq:affinity}
    A_{ij}=A_{ji}=Sim(S_i,S_j)
\end{equation}

Here, $S_i, S_j$ are sentences from the input document. The affinity matrix, $A$, is used in the spectral clustering which is implemented using SciKit-learn library~\cite{Pedregosa-2011-scikit-learn} of python. It is also necessary to provide the number of clusters to achieve. The number of clusters is fixed at $k=ceiling\left(\frac{N}{5}\right)$ due to it being a reasonable size to contain all necessary sentences as well as being short enough to be an effective summary.

\subsection{Summary Generation}\label{subsec:summary-generation}
Summarized text is the collection of selected sentences from different clusters. After clustering, we pick one sentence from each cluster. The sentences inside a cluster are ranked among themselves using TF-IDF techniques. From each cluster, the sentence with the most TF-IDF score is selected. We then rearranged these picked sentences are in their order of appearance to retain the normal flow of information in the input. These sentences are then concatenated together to produce the final output summary. The clustering and summary generation process is shown in Algorithm~\ref{alg:summary}. After clustering in the lines 1 and 2, We ranked the sentences in the lines 3--8. The best sentence indexes are picked in the lines 9--11. The summary is generated in the lines 12--16.

\begin{algorithm} \caption{Summary Generation} \label{alg:summary}
\begin{algorithmic}[1]
    \State $k \gets \lceil$ length($A$) / 5 $\rceil$
    \State clusters $\gets$ spectral\_clustering(adjacency = $A$, $k$)
    \State indexes $\gets \{\}$
    \For{each cluster$_i$ in clusters}
        \State TFIDF $\gets \{\}$
        \For{each index in cluster$_i$}
            \State TFIDF.append(tfidf\_sum(sentences[index]))
        \EndFor
        \State indexes.append(indexof(max(TFIDF)))
    \EndFor
    \State sort(indexes)
    \State $S \gets `` "$
    \For{each $i$ in indexes}
        \State $S \gets S +$ sentences[$i$]
    \EndFor
    \State \textbf{Return} $S$
\end{algorithmic}
\end{algorithm}

