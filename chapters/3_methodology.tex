The summarization process followed here can be boiled down as two basic steps, grouping all the close sentences together based on their meaning to minimize redundancy and picking one sentence from each group to maximize sentence coverage. To group semantically similar sentences into clusters, we build a sentence similarity graph and perform spectral clustering on it~\cite{roychowdhury-etal-2022-spectral-base}. The sentence similarity graph is produced using a novel sentence similarity calculation algorithm that uses geometric mean of Gaussian similarity between individual word pairs from the two sentences. The Gaussian similarity is calculated using the vector embedding representations of the words. On the other hand, we used TF-IDF scores of the sentences in a cluster to pick the highest ranked and thus most representative sentence from the cluster~\cite{Akter-2017-tfidf-3,das-2022-tfidf,sarkar-2012-tfidf,sarkar-2012-tfidf-2}. The summarization process followed here involves 4 steps. These are, Pre-processing, Sentence similarity calculation, Clustering and Summary generation. These steps are further discussed in the following subsections. 
%Most of the extractive summarization methods involve scoring the sentences based on a sentence scoring metric (generally a version of TF-IDF) and then picking the best scoring sentences to generate the summaries~\cite{Akter-2017-tfidf-3,das-2022-tfidf,sarkar-2012-tfidf,sarkar-2012-tfidf-2}. But our method, firstly, grouped the sentences with similar meaning together and then picked one sentence from each group to generate a summary. This will ensure maximum coverage of topics while also reducing redundancy. But the main challenge to reducing the redundancy is to develop a method that can accurately predict how close the meaning of the two sentences is. In this paper, we propose a method that can more accurately determine sentence similarity. The summarization process followed here involves 4 steps. These are, Pre-processing, Sentence similarity calculation, Clustering and Summary generation. These steps are further discussed in the following subsections.

\subsection{Preprocessing}\label{subsec:preprocessing}
Preprocessing is the standard process of NLP that transforms raw human language inputs into a format that can be used by a computer algorithm. In this preprocessing step, the input document is transformed into a few set of vectors where each word is represented with a vector, each sentence is represented as a set of vectors and the whole document as a list containing those sets. To achieve this representation, the preprocessing follows three steps. These are tokenization, stop word removal, and word embedding. A very common step in preprocessing, word stemming, isn't used here as the word embedding dataset works best for the whole word instead of the root word. These steps are further discussed below.\\

Tokenization is the step of dividing an input document into sentences and words to transform it into a more usable format. Here, the input document is represented as a list of sentence and the sentences are represented as a list of words. Stop words, such as prepositions and conjunctions, add sentence fluidity but donâ€™t carry significant meaning. Removing these words allows the algorithm to focus on more impactful words. To remove these stop words, we used a list\footnote{\textit{https://www.ranks.nl/stopwords/bengali}} of 363 bengali words. Word Embedding is the process of representing  words as vector in a vector space. In this vector space, semantically similar words are placed closer together so that the similarity relation between words can be expressed in an abstract mathmatical way. We used the FastText dataset\footnote{\textit{https://fasttext.cc/docs/en/crawl-vectors.html}}~\cite{grave-etal-2018-fasttext} with 1.47 million Bengali words and their vector representation to achieve this step. Each word from the tokenized and filtered list is replaced with their corresponding vectors. If some words aren't present in the dataset, they are considered too rare and ignored. Following these steps, the input document is transformed into a set of vectors to be used in sentence similarity calculation. 

\subsection{Sentence Similarity Calculation}\label{subsec:sentence-similarity-calculation}
Sentence similarity is the key criteria to build a graphical representation of the relation between the sentences in the input document. This graphical representation is expressed via an affinity matrix to cluster the sentences into groups of semantically similar sentences. The nodes in the affinity matrix represents the sentences of the input and the edges of the matrix represents the similarity between two sentence to graphically represent the input document. Here, we proposed a novel sentence similarity calculation technique using individual Gaussian similarity between close word pairs to construct the affinity matrix. To calculate the sentence similarity between two sentences, we adhere to the following steps.\\

Firstly, for every word of a sentence, we find its closest counterpart from the other sentence to build a word pair. The Euclidean distance between the vector representation of the two words from this pair is defined as the Most Similar Word Distance ($D_{msw}$) to be used in the following steps. The process of calculating the $D_{msw}$ is shown in the equation \ref{eq:msd}. In this equation, for every word vector $x$, in a sentence $X$, we find the Euclidean distance ( $d(x,y_i)$ ) between the word vectors $x$ and $y_i$ where $y_i$ is a word vector from the sentence $Y$. The lowest possible distance in this set of Euclidean distance is the $D_{msw}$. 
\begin{equation}\label{eq:msd}
    D_{msw}(x,Y) = min(\{d(x,y_i) : y_i \in Y \})
\end{equation}
Then, we calculate the $D_{msw}$ for every word of the two sentences $X$ and $Y$ to make the sentence similarity calculation symmetric. This process is shown in the equation \ref{eq:mswdset}. In this equation $D$, is a set containing all the $D_{msw}$ from both $X$ and $Y$ that would be used in the later steps.
\begin{equation}
    D = \{D_{msw}(x,Y) : x \in X\} \cup \{D_{msw}(y,X) : y \in Y\}
    \label{eq:mswdset}
\end{equation}
The word similarity is calculated using Gaussian similarity for each of the element of $D$. The equation for word similarity is shown in Equation~\ref{eq:wsim}.

\begin{equation}\label{eq:wsim}
    W_{sim} = \{ exp\left(\frac{-D_i^2}{2\sigma^2}\right) : D_i \in D\}
\end{equation}

Here, $\sigma$ denotes the standard deviation which represents how much noise sensitive the similarity should be. A smaller $\sigma$ represents a highly noise sensitive similarity. This is used as a control variable to further fine tune the algorithm. The similarity between the two sentence, $Sim(X,Y)$, is calculated by taking the Geometric mean of all the word similarities from both sentences. Taking $D_{msw}$ from both sentences makes the $Sim(X,Y)$ value symmetrical as this would result in $Sim(X,Y) = Sim(Y,X)$. The process of getting $Sim(X,Y)$ is explained in the following Equation~\ref{eq:sent_sim}

\begin{equation}\label{eq:sent_sim}
    \begin{split}
        Sim(X,Y)
        &=  \left(
                \prod_{i=1}^nW_{Sim_i}
            \right)^{\frac{1}{n}}\\
        &=  \left(
                e^{\frac{-D_1^2}{2\sigma^2}}\cdot
                e^{\frac{-D_2^2}{2\sigma^2}}\cdot
                    \ldots \cdot
                e^{\frac{-D_n^2}{2\sigma^2}}
            \right)^\frac{1}{n}\\
        &=  exp\left(
                -\frac{D_1^2+D_2^2+\ldots+D_n^2}{2n\sigma^2}
            \right)\\
        &=  exp\left(
                -\frac{\sum_{i=1}^nD_i^2}{2n\sigma^2}
            \right)
    \end{split}
\end{equation}

Here, by taking geometric means of the similarity between the closest words together in two sentences, an effective word to word comparison has been created between those sentences. This reduces any misleading distance that would have come from the word averaging method due to the tendency towards center. An example of this solution is depicted through Figure~\ref{fig:msd}. Here, a more representative word association can be seen for both scenarios from Figure~\ref{fig:sarkar-problem}. Red and Blue dots in the figure represent two sets of word vectors in a sentence pair. Black-dashed lines show the Most Similar Word Distance, $(D_{msw}(x,Y))$, for a word vector $x$ and the other sentence $Y$. The arrowheads point from $x$. The Figure~\ref{fig:msd}(a) shows the $D_{msw}$ for Scenario A in Figure~\ref{fig:sarkar-problem}(a). The Figure~\ref{fig:msd}(b) Shows the $D_{msw}$ for Scenario B in Figure~\ref{fig:sarkar-problem}(b). We can see that the sentences with closer words have smaller $D_{msw}$s and would have smaller geometric mean than the sentences with words that are farther apart. So the problem caused by the averaging method has been mitigated here.\\

The standard deviation $\sigma$ in the Equation \ref{eq:sent_sim} was fine-tuned to be $5\times10^{-11}$ where it gave the best results (Figure~\ref{fig:sigma-fine-tuning}). The process of building the affinity matrix, $A$, is described in the Algorithm~\ref{alg:similarity}. Here, line 9--13 and 19--23 are the process of getting $D_{msw}$. Line 7--26 describes the process of getting $\sum^n_{i=1}D_i^2$. Line 4--29 describes the process of getting $Sim(Sentence_i,Sentence_j)$. Line 1--31 describes the process of getting the affinity matrix, $A$.

\begin{figure}
    \centering
    \input{figs/0301_msd}
    \caption{Process of obtaining $D_{msw}$}
    \label{fig:msd}
\end{figure}
\begin{algorithm} \caption{Sentence Similarity Calculation} \label{alg:similarity}
\begin{algorithmic}[1]
    \State $n \gets$ length(WordVectorList)
    \State $A \gets \{ \{0\} \times n \} \times n$
    \For{each sentence$_i$ in WordVectorList}
        \State $D_{\text{Square}} \gets 0$
        \State count $\gets 0$
        \For{each sentence$_j$ in WordVectorList}
            \For{each word$_i$ in sentence$_i$}
                \State $D_{\text{msw}} \gets \infty$
                \For{each word$_j$ in sentence$_j$}
                    \If{Distance(word$_i$, word$_j$) $< D_{\text{msw}}$}
                        \State $D_{\text{msw}} \gets$ Distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{\text{Square}} \gets D_{\text{Square}} + D_{\text{msw}}^2$
                \State count++
            \EndFor
            \For{each word$_j$ in sentence$_j$}
                \State $D_{\text{msw}} \gets \infty$
                \For{each word$_i$ in sentence$_i$}
                    \If{Distance(word$_i$, word$_j$) $< D_{\text{msw}}$}
                        \State $D_{\text{msw}} \gets$ Distance(word$_i$, word$_j$)
                    \EndIf
                \EndFor
                \State $D_{\text{Square}} \gets D_{\text{Square}} + D_{\text{msw}}^2$
                \State count++
            \EndFor
            \State similarity $\gets \exp \left( \frac{- D_{\text{Square}}}{2 \times \text{count} \times \sigma^2} \right)$
            \State $A[i][j] \gets A[j][i] \gets$ similarity
        \EndFor
    \EndFor
    \State \textbf{Return} $A$
\end{algorithmic}
\end{algorithm}

\subsection{Clustering}\label{subsec:clustering}
The clustering is the most integral part of this summarization technique, aiming to group all the sentences with similar meanings together. Here, spectral clustering is used to cluster the sentences using sentence similarity calculated in the step above. Spectral clustering was chosen here because \citeauthor{roychowdhury-etal-2022-spectral-base} \cite{roychowdhury-etal-2022-spectral-base} found it to be better performing than DBSCAN method. The spectral clustering steps were followed according to the tutorial given by \cite{vonLuxburg-2007-spectral-tutorial}. \\

To perform spectral clustering on a data, firstly, an affinity matrix is required that shows the weight of edges between the vertexes in the graph. Here the affinity $A$ is prepared using the following Equation~\ref{eq:affinity}.

\begin{equation}\label{eq:affinity}
    A_{ij}=A_{ji}=Sim(S_i,S_j)
\end{equation}

Here, $S_i, S_j$ are sentences from the input document. The affinity matrix, $A$, is used in the spectral clustering which is implemented using SciKit-learn library~\cite{Pedregosa-2011-scikit-learn} of python. It is also necessary to provide the number of clusters to achieve. The number of clusters is fixed at $k=ceiling\left(\frac{N}{5}\right)$ due to it being a reasonable size to contain all necessary sentences as well as being short enough to be an effective summary.

\subsection{Summary Generation}\label{subsec:summary-generation}
Summarized text is the collection of selected sentences from different clusters. After clustering, we pick one sentence from each cluster. The sentences inside a cluster are ranked among themselves using TF-IDF techniques. From each cluster, the sentence with the most TF-IDF score is selected. We then rearranged these picked sentences are in their order of appearance to retain the normal flow of information in the input. These sentences are then concatenated together to produce the final output summary. The clustering and summary generation process is shown in Algorithm~\ref{alg:summary}. After clustering in the lines 1 and 2, We ranked the sentences in the lines 3--8. The best sentence indexes are picked in the lines 9--11. The summary is generated in the lines 12--16.

\begin{algorithm} \caption{Summary Generation} \label{alg:summary}
\begin{algorithmic}[1]
    \State $k \gets \lceil$ length($A$) / 5 $\rceil$
    \State clusters $\gets$ spectral\_clustering(adjacency = $A$, $k$)
    \State indexes $\gets \{\}$
    \For{each cluster$_i$ in clusters}
        \State TFIDF $\gets \{\}$
        \For{each index in cluster$_i$}
            \State TFIDF.append(tfidf\_sum(sentences[index]))
        \EndFor
        \State indexes.append(indexof(max(TFIDF)))
    \EndFor
    \State sort(indexes)
    \State $S \gets `` "$
    \For{each $i$ in indexes}
        \State $S \gets S +$ sentences[$i$]
    \EndFor
    \State \textbf{Return} $S$
\end{algorithmic}
\end{algorithm}

